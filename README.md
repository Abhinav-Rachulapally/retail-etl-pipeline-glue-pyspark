# Retail ETL Pipeline with AWS Glue & PySpark

This project demonstrates an end-to-end ETL pipeline for retail sales data using AWS Glue, S3, PySpark, and Redshift.

## ğŸ§° Tech Stack
- AWS Glue (ETL orchestration)
- PySpark
- Amazon S3
- Amazon Redshift
- AWS IAM
- Python

## ğŸš€ Pipeline Overview

1. **Data Ingestion**: Raw sales data uploaded to S3 bucket
2. **Transformation**: AWS Glue job runs PySpark to clean & transform data
3. **Loading**: Final data saved to Redshift in Parquet format
4. **Analysis**: Ready for Power BI, Redshift queries, or Athena

## ğŸ“Š Reporting

The transformed data from Redshift was analyzed using Power BI dashboards.
Metrics included:

- Daily sales trends
- Top-selling products
- Store-wise performance


## ğŸ“Š Sample Use Case

- Dataset: 1M+ sales records across 50 stores
- Replaced Excel-based reporting â†’ reduced latency by ~80%

## ğŸ“ Folder Structure
retail-etl-pipeline-glue-pyspark/
â”‚
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ retail_glue_job.py          â† PySpark job to clean & transform retail data
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ sample_sales.csv            â† Sample input data (optional)
â”‚
â”œâ”€â”€ dashboard/
â”‚   â””â”€â”€ retail_dashboard.png        â† Screenshot of Power BI or other report (optional)
â”‚
â””â”€â”€ README.md                       â† Project overview, tech stack, pipeline steps, and author


## ğŸ™‹â€â™‚ï¸ Author
Abhinav Rachulapally  
Data Engineer with 2+ years experience in building data pipelines
