# Retail ETL Pipeline with AWS Glue & PySpark

This project demonstrates an end-to-end ETL pipeline for retail sales data using AWS Glue, S3, PySpark, and Redshift.

## ğŸ§° Tech Stack
- AWS Glue (ETL orchestration)
- PySpark
- Amazon S3
- Amazon Redshift
- AWS IAM
- Python

## ğŸ“ Folder Structure
retail-etl-pipeline-glue-pyspark/
â”‚
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ retail_glue_job.py
â”‚
â”œâ”€â”€ dashboard/
â”‚   â””â”€â”€ sales_dashboard.png   (optional)
â”‚
â””â”€â”€ README.md

## ğŸš€ Pipeline Overview

1. **Data Ingestion**: Raw sales data uploaded to S3 bucket
2. **Transformation**: AWS Glue job runs PySpark to clean & transform data
3. **Loading**: Final data saved to Redshift in Parquet format
4. **Analysis**: Ready for Power BI, Redshift queries, or Athena

## ğŸ“Š Reporting

The transformed data from Redshift was analyzed using Power BI dashboards.
Metrics included:

- Daily sales trends
- Top-selling products
- Store-wise performance


## ğŸ“Š Sample Use Case

- Dataset: 1M+ sales records across 50 stores
- Replaced Excel-based reporting â†’ reduced latency by ~80%

## ğŸ™‹â€â™‚ï¸ Author
Abhinav Rachulapally  
Data Engineer with 2+ years experience in building data pipelines
